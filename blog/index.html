<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}
	</style>

	<title>Convbrid Transformer: Enforcing Locality via CNN-like Attention Masking</title>
	<meta property="og:title" content="The Platonic Representation Hypothesis" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							Convbrid Transformer: Enforcing Locality via CNN-like Attention Masking
						</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px">Vinh Tran</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Joel Tan</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Toya Takahashi</span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#convbrid">Convbrid Architecture</a><br><br>
				<a href="#experiments">Experiments</a><br><br>
				<a href="#results">Results</a><br><br>
				<a href="#further_work">Further Work</a><br><br>
				<a href="#conclusion">Conclusion</a><br><br>
			</div>
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Introduction</h2>
            Vision Transformers (ViTs) adapt the standard transformer architecture, originally designed for natural
            language processing, to computer vision tasks. They process images by dividing them into patches and treating
            these patches as a sequence of inputs to the transformer. ViTs have demonstrated improved performance compared
            to deep convolutional neural networks (CNNs) on large-scale datasets such as ImageNet-21k and JFM-300M. <a href="#ref_1">[1]</a>
            <br><br>

            The self-attention mechanism in Vision Transformers (ViTs) enables them to capture global image context,
            enhancing performance. However, unlike convolutional neural networks (CNNs), ViTs lack inductive biases for learning
            local structures and are not inherently translation invariant due to their reliance on positional embeddings. As
            a result, ViTs require large datasets or extensive data augmentation for effective training. This also leads to
            a higher parameter count and the need for substantially larger training datasets to match the performance of CNNs.
            Notably, when trained on mid-sized datasets without regularization, ViTs perform comparably or slightly worse
            than ResNets of similar size <a href="#ref_1">[1]</a>. When trained on CIFAR-100, ViTs tend to overfit the training set due to their difficulty in capturing local information, resulting in lower test accuracy <a href="#ref_9">[9]</a>:<br><br>

            <img src="./images/vit_vs_cnn_li.jpg" width=512px/>

            While substantial research has focused on developing accurate models that require large datasets,
            relatively little attention has been given to training models on small datasets. This poses a
            significant challenge for developing effective ViT models in domains with limited
            large-scale datasets, such as medical imaging <a href="#ref_7">[7]</a>.<br></br>

            In this project, the tendency of ViTs to overfit and struggle with learning local information was validated
            on small-scale datasets, using CIFAR-10 and CIFAR-100. Then, to address this issue, the Convbrid
            Transformer was developed. This is a custom hybrid CNN-ViT architecture that incorporates
            locality inductive biases. During the initial stages of training, Convbrid uses attention masks to limit attention
            to local patches, mimicking the locality behavior of CNNs. This constraint encourages the model to learn spatial locality
            early on during training without the need of large datasets for the model to learn inductive biases.
            By combining the spatial awareness of convolutional layers with the global attention capabilities of
            transformers, this design facilitates more efficient training on smaller datasets.<br><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="bg_and_rel_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background and Related Work</h1>
            Some prior work has aimed to address the lack of inductive bias in Vision Transformers (ViTs). For instance, Li
            et al. proposed adding a small multi-layer perceptron (MLP) to the architecture to penalize pairs of token embeddings
            with large relative distances <a href="#ref_6">[6]</a>. Since the MLP functions independently of the original
            architecture, it is inherently architecture-agnostic. Results show that this additional loss consistently enhances
            the accuracy of ViTs across all tested datasets, with improvements ranging from modest to significant. However,
            a notable limitation is that the MLP constrains the transformer from fully leveraging its capacity to learn
            global relationships. By being active throughout the training process, the MLP may inhibit the ViT's ability
            to exploit its full potential for modeling long-range dependencies.<br><br>

            Another approach involves using a small CNN as a regularizer to guide the local feature learning of Vision Transformers
            (ViTs) <a href="#ref_9">[9]</a>. In this method, a lightweight, pre-trained CNN operates in parallel with the downsampled
            image alongside the transformer layers. After each transformer layer, the architecture minimizes the L2 loss between the
            output of the CNN and that of the transformer. This process distills locality knowledge from the CNN into the ViT’s hidden
            layers during training, with the CNN outputs being discarded during inference. Consistent with previous studies, results
            show that on small datasets such as CIFAR-100, this approach leads to up to a 13% improvement over the baseline transformer model.
            <br><br>

            Lastly, a notable advancement in tackling this challenge is the T2T-ViT (Tokens-to-Token Vision Transformer) architecture,
            which combines multi-layer perceptrons (MLPs) for progressive tokenization of image structures with an efficient CNN-inspired
            deep-narrow backbone to model global attention relations on tokens <a href="#ref_7">[7]</a>. This dual approach enables the T2T-ViT
            to effectively capture local image details while simultaneously leveraging global context through attention mechanisms.
            By combining these capabilities, the T2T-ViT demonstrates superior or comparable performance to CNNs of similar complexity,
            such as ResNets and MobileNets. This highlights the importance of incorporating locality guidance into Vision Transformers,
            addressing their inherent lack of inductive bias and improving their efficiency in learning from midsize datasets.<br><br>

            Compared to previous methods, our Convbrid architecture offers a more direct imitation of CNNs by initially restricting
            attention to local patches. However, as training progresses, the attention masks are removed, allowing the model to
            integrate global information alongside the local features learned in the earlier stages of training. We hypothesize that
            this novel training approach will enable the transformer to learn local features more quickly than traditional ViTs,
            without compromising its ability to capture global dependencies. This method aims to find a balance between efficiently learning
            locality and maintaining the transformer's capacity for global attention.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);">
		    <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="convbrid">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Convbrid Architecture</h2>
			
			The Convbrid transformer aims to emulate the behavior of extracting data from an image patch by performing convolution
            of a given kernel size as in a traditional CNNs. To this end, we developed the ConvAttention layer,
			the attention mechanism in Convbrid was modified to operate in a localized manner, which resembles the small
			receptive field in a convolutional kernel. Thus, each attention layer in the Convbrid is masked, to create a ConvAttention layer.
            In a similar manner to a convolutional layer, a kernel size 
			can be chosen. This defines the size of the "neighborhood" of tokens that a given token can attend to. Here, each token represents
			a patch in the original image. 
			<p>
				If 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>k</mi>
				</math>
				is the kernel size, then each token 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>t</mi>
				</math>
				interacts with the nearby tokens 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>N</mi><mo>(</mo><mi>t</mi><mo>)</mo>
				</math>, which are the tokens corresponding to the patches in its 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>k</mi>
					<mo>&#xD7;</mo>
					<mi>k</mi>
				</math>
				neighborhood (including padding tokens).
			</p>
			<p>
				The stride can be similarly defined, where for only a subset of tokens 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>t</mi>
				</math>
				is 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mi>N</mi><mo>(</mo><mi>t</mi><mo>)</mo>
				</math>
				considered. This subset corresponds to the centers of the kernels in the 
				corresponding convolutional layer with the same stride.
			</p>
			<p>
				After the masked self-attention, there is a pooling layer to pool the (up to 
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<msup>
						<mi>k</mi>
						<mn>2</mn>
					</msup>
				</math>
				) attention scores for each token.
			</p>
		 This is followed by a multi-layer perceptron (MLP). Throughout the rest of 
			this discussion, a ConvAttention layer refers to one attention layer with its pooling layer and MLP. <br></br>

			It is noted that the pooling layer itself enforces locality. However, this architecture still fundamentally differs from a CNN 
			in that it dynamically determines the importance of interactions within the kernel rather than applying fixed, learned filters in CNNs.
			This enables the ConvAttention layer to adapt focus based on the input patches. <br></br>

			Need to mention when ConvAttention is used, position embeding is not necessary, as the attention masking already capture the positional information.

			<img src="./images/model.png" width="256px">

			In order to harness the strengths of both ViTs (in global context understanding) and CNNs (in local patterns), Convbrid
			starts training with a small kernel size, and then the mask is removed after some epochs (which can also be set) so that 
			it functions like a transformer.<br></br>

	
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="experiments">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Experiments</h2>
			<h1>Verifying CNN and Transformer</h1>
            A series of experiments is first performed to validate the findings of previous papers that ViTs struggle to
            find meaningful relations between image patches close to each other and tend to overfit to the test data.<br><br>

            <!--
			- Repeat the test in CIFAR 10 and CIFAR 100.
            - Describe our models and have visuals
            - Expected behavior
            - Learn locality -> learn attention better because we are focusing our attention on limited places
            - CIFAR10 and CIFAR100
            - Mechanism of releasing the masking
            - Discuss our results (why the frick did nothing change)
            - Train a bit faster
            - Not able to repeat the test of tiny transformer paper because
            a lot of resources are needed to repeat the results of the paper
            - How we do the masking. Reflecting the behavior of CNN to convert transformer to CNN.
			-->
			
            The test in <a href="#ref_9">[9]</a> is repeated, comparing the performances of a <!--T2T-ViT-14--> transformer
            <a href="#ref_7">[7]</a> to that of a CNN with the same complexity using the CIFAR-10 and CIFAR-100 datasets 
			(These are small datasets, with training and test datasets containing 50000 and 10000 images respectively). 
			We aim to quantify the level of improvement with CNN over transformers in the context of these 
			small datasets, and confirm that the CNN will outperform the transformer. <br></br>

			The bench marking CNN is designed to allow the creation of equivalent architectures using ConvAttention layers. 
			The networks are composed of an initial embedding layer, implemented using Torch's built-in
			Conv2D layers with the kernel size of 3 and the stride and padding of 1. This directly map 1024 pixels to 1024 initial tokens. The embedding size of the token is chosen to be 64
			and this is kept throughout the layers (as number of input and output channels to the convolutional layers). The tokens are then passed through 6 convolution blocks as described bellow. For the CNN we built the convolution blocks with two simple convolution layers
			with MLP layers in between and afterwards. This is to make traditional convolution block perform and have the same complexity as the ConvAttention block. <br></br>
			
			<table>
				<thead>
					<tr>
						<th>Layer</th>
						<th>Kernel Size</th>
						<th>Stride</th>
						<th>Padding</th>
						<th>Output Size</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td>1</td>
						<td>5</td>
						<td>2</td>
						<td>2</td>
						<td>16 × 16</td>
					</tr>
					<tr>
						<td>2</td>
						<td>5</td>
						<td>2</td>
						<td>2</td>
						<td>8 × 8</td>
					</tr>
					<tr>
						<td>3</td>
						<td>5</td>
						<td>1</td>
						<td>2</td>
						<td>8 × 8</td>
					</tr>
					<tr>
						<td>4</td>
						<td>5</td>
						<td>1</td>
						<td>2</td>
						<td>8 × 8</td>
					</tr>
					<tr>
						<td>5</td>
						<td>5</td>
						<td>2</td>
						<td>2</td>
						<td>4 × 4</td>
					</tr>
					<tr>
						<td>6</td>
						<td>5</td>
						<td>2</td>
						<td>2</td>
						<td>2 × 2</td>
					</tr>
				</tbody>
			</table>
			
			For the transformer, TO TRAIN ACTUAL TRANSFORMER WITHOUT LOCALITY <br><br>

			Each model is run until saturation, using the AdamW optimizer and Cosine Annealing scheduler. <br><br>

			<h1>ConvAttention v.s. CNN</h1>
			To verify the functionality of the ConvAttention layer, models created using ConvAttention and CNN layers with
            the same complexity (explain what same complexity mean) ar compared. <br><br>

			We except similar behaviour and validation accuracy. Although ConvAttention take more time to train. <br><br>

			Discuss the attention visualization of the ConvAttention model <br><br>

			<h1>Convbrid Transformer</h1>
			A Convbrid transformer was then created with the exact convolution characteristics of the CNN (similar
            layers and kernel sizes) and compare its performance on the CIFAR-10 and CIFAR-100 datasets with the two aforementioned models. 
			The Convbrid model is expected to perform similarly if not better than the CNN.<br><br>

            After this initial step, the aim is to explore different ways of manipulating the Convbrid kernel sizes, and 
			analyse how they affect performance, expanding to larger datasets such as ImageNet-1K <a href="#ref_10">[10]</a><br>.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Results</h2>

			<h1>Evaluating the CNN and Transformer</h1>
			The CNN was trained on CIFAR-10 for 40 epochs and CIFAR-100 for 150 epochs. The number of epochs was determined by
			how long it takes for the training accuracy to plateau, which is determined by when 2 epochs do not change
			training accuracy by 1%. 

			<img src="./images/training_stats.png" width=1024px/>
			The above figure demonstrates the performances of the various models on CIFAR-10 and CIFAR-100. The 
			accuracies shown in the figure are the best validation accuracy up to that point. As expected, the CNN outperforms the
			transformer.
			
			<h1>ConvAttention</h1>
			The attention masks were visualized INSERT IMAGES and it was found that it resembles a convolutional layer in enforcing locality.
			Higher attention appears to be given to tokens near edges or areas with abrupt color transitions. This suggests that
			the attention mechanism is inherently able to detect significant local features, just as in a CNN. <br></br>

			<img src="./images/conv_attention_visualization_cifar10.png" width=512px/>

			<h1>ConvBrid</h1>
			Discussing the attention visualization of the ConvBrid model <br></br>
			
			INSERT FIGURE <br></br>

			It was found that the switching mechanism does not work as intended. Discuss the TradAttention model, explaining that is the ConvAttention without attention masking (but with pooling). This model behavae exactly like the ConvAttention.
			Discuss that the restricting mechanism is satisfy with the pooling alone and the attention masking is not necessary. This is still an interesting result. However, it create a diffuculty in transitioning between ConvAttention and traditional transfomer. <br></br>

			During the training of the ConvBrid model, the transition from ConvAttention (with kernel size = 5) to a transformer (without masking) 
			is triggered when the ConvAttention module reaches saturation, indicated by the training accuracy nearly plateauing. For CIFAR-10 and 
			CIFAR-100, this corresponds to epochs 15 and 80 respectively, as represented in the figure above.<br></br>

			</div>
		<div class="margin-right-block">
		</div>
	</div>

	<!-- REMOVE IF THERES ENOUGH THINGS TO SAY-->
	<div class="content-margin-container" id="further_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Further Work</h2>
			A further attempt was to change the ConvBrid kernel sizes more fluidly during the training process. The changed ConvBrid
			model takes as input a function from epoch to kernel size, so that by inputting a slowly growing function,
			the model will learn progressively broader contextual information. It aims to mimic a hierarchical learning process which
			will be more efficient. <br></br>

			However, as the pooling layer in the ConvAttention needs to have a fixed structure across epochs, it has to be initially set
			to be compatible with the maximum kernel size in the training process. While in the static case the dimension of the 
			input is <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>, now it has to be set to <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>max_kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>, if the current kernel size
			is less than the corresponding weights, the tokens outside the kernel will not be updated in the training pass. Thus, a custom
			backward pass in the linear pooling layer was implemented to enable the layer to do this selective dropout. <br></br>
			
			As of now, the model set at kernel = 5 is only able to reach 52% accuracy. This probably points to further debugging that has to be
			done. Theoretically, it should be able to perform comparably to the initial ConvBrid model, if not better. <br></br>
			
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Conclusion</h2>
			Let's end with some discussion of the implications and limitations.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:16px">References:</span><br><br>
				<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">
					An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>,
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
                    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 2021<br><br>
				<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2006.04768">
                    Linformer: Self-Attention with Linear Complexity</a>,
                    S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, 2020<br><br>
				<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1904.10509">
                    Generating Long Sequences with Sparse Transformer</a>,
                    R. Child, S. Gray, A. Radford, I. Sutskever, 2019<br><br>
				<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/1911.02972">
                    Blockwise Self-Attention for Long Document Understanding</a>,
                    J. Qiu, H. Ma, O. Levy, S. Wen-tau Yih, S. Wang, J. Tang, 2019<br><br>
				<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2004.05150">
                    Longformer: The Long-Document Transformer</a>,
                    I. Beltagy, M. E. Peters, A. Cohan, 2020<br><br>
				<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2106.03746">
                    Efficient Training of Visual Transformers with Small Datasets</a>,
                    Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. De Nadai, 2021<br><br>
				<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/2101.11986">
                    Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</a>,
                    L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng, S. Yan, 2021<br><br>
				<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2104.05704">
                    Escaping the Big Data Paradigm with Compact Transformers</a>,
                    A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, H. Shi, 2021<br><br>
				<a id="ref_9"></a>[9] <a href="https://arxiv.org/abs/2207.10026">
                    Locality Guidance for Improving Vision Transformers on Tiny Datasets</a>,
                    K. Li, R. Yu, Z. Wang, L. Yuan, G. Song, J. Chen, 2022<br><br>
			    <a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/document/5206848">
                    Imagenet: A large-scale hierarchical image database</a>,
                    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 2009<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>
