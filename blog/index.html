<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}
	</style>

	<title>Convbrid Transformer: Enforcing Locality via CNN-like Attention Masking</title>
	<meta property="og:title" content="The Platonic Representation Hypothesis" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							Convbrid Transformer: Enforcing Locality via CNN-like Attention Masking
						</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px">Vinh Tran</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Joel Tan</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Toya Takahashi</span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<!-- table of contents here -->
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#does_x_do_y">Does X do Y?</a><br><br>
				<a href="#implications_and_limitations">Implications and limitations</a><br><br>
			</div>
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Introduction</h1>
            Vision Transformers (ViTs) adapt the standard transformer architecture, originally designed for natural
            language processing, to computer vision tasks. They process images by dividing them into patches and treating
            these patches as a sequence of inputs to the transformer. ViTs have demonstrated improved performance compared
            to deep convolutional neural networks (CNNs) on large-scale datasets such as ImageNet-21k and JFM-300M. <a href="#ref_1">[1]</a>
            <br><br>

            The self-attention mechanism in Vision Transformers (ViTs) enables them to capture global image context,
            enhancing performance. However, unlike convolutional neural networks (CNNs), ViTs lack inductive biases for learning
            local structures and are not inherently translation invariant due to their reliance on positional embeddings. As
            a result, ViTs require large datasets or extensive data augmentation for effective training. This also leads to
            a higher parameter count and the need for substantially larger training datasets to match the performance of CNNs.
            Notably, when trained on mid-sized datasets without regularization, ViTs perform comparably or slightly worse
            than ResNets of similar size <a href="#ref_1">[1]</a>. When trained on CIFAR-100, ViTs tend to overfit the training set due to their difficulty in capturing local information, resulting in lower test accuracy <a href="#ref_9">[9]</a>:<br><br>

            <img src="./images/vit_vs_cnn_li.jpg" width=512px/>

            While substantial research has focused on developing accurate models that require large datasets,
            relatively little attention has been given to training models on small datasets. This poses a
            significant challenge for developing effective ViT models in domains with limited
            large-scale datasets, such as medical imaging <a href="#ref_7">[7]</a>.<br><br>

            In this project, we began by validating the tendency of ViTs to overfit and struggle with learning local information
            on small-scale datasets, using CIFAR-10 and CIFAR-100. Then, to address this issue, we attempted to develop the Convbrid
            Transformer, a custom hybrid CNN-ViT architecture that incorporates
            locality inductive biases. During the initial stages of training, Convbrid uses attention masks to limit attention
            to local patches, mimicking the locality behavior of CNNs. This constraint encourages the model to learn spatial locality
            early on during training without the need of large datasets for the model to learn inductive biases.
            By combining the spatial awareness of convolutional layers with the global attention capabilities of
            transformers, this design facilitates more efficient training on smaller datasets.<br><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="does_x_do_y">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background and Related Work</h1>
            Some prior work has been done on mitigating the lack of inductive bias on ViTs. For instance, Li et. al proposed adding a small
            multi-layer perception (MLP) <!-- TODO WHAT THE FRICK IS GOING ON -->
            to the architecture to learn relative distances in the image from the patch embeddings <a href="#ref_6">[6]</a> has been
            shown to improve performance. Using MLPs to successively tokenize the image is another way to capture local
            structures <a href="#ref_7">[7]</a>. Convbrid will provide a closer imitation of the CNNs with the desirable inductive
            bias. It has also been shown that careful tokenization can overcome the need for large datasets in
            transformers. <a href="#ref_8">[8]</a>. 
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);">
		    <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="Convbrid">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Convbrid Architecture</h1>
			The Convbrid transformer aims to emulate the behavior of the convoluational layers in traditional CNNs. To this end,
			the attention mechanism in Convbrid needs to be modified to operate in a localized manner, which resembles the small
			receptive field in a convolutional kernel. <br><br>
			
			Thus, each attention layer in the Convbrid is masked. In a similar manner to a convoluational layer, a kernel size 
			is set. This defines the size of the "patch" of tokens that a given token can attend to. Here, each token represents
			a patch in the original image. If k is the kernel size, then each token interacts with the tokens corresponding to the
			k x k neighborhood (except boundary tokens). After the masked self-attention, there is a pooling layer to pool the (up to k^2) attention
			scores for each token. This is followed by a multi-layer perceptron.  <br><br>

			INSERT VISUAL FOR ATTENTION MASK<br><br>

			INSERT VISUAL FOR ATTENTION LAYER<br><br>

			In order to harness the strengths of both ViTs (in global context understanding) and CNNs (in local patterns), Convbrid
			starts training with a small kernel size, and then the mask is removed after some epochs (which can also be set) so that 
			it functions like a transformer.<br><br>
	
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Experiments</h1>
            We first performed a series of experiments to validate the findings of previous papers that ViTs struggle to
            find meaningful relations between image patches close to each other and tend to overfit to the test data.<br><br>

            <!--
			- Repeat the test in CIFAR 10 and CIFAR 100.
            - Describe our models and have visuals
            - Expected behavior
            - Learn locality -> learn attention better because we are focusing our attention on limited places
            - CIFAR10 and CIFAR100
            - Mechanism of releasing the masking
            - Discuss our results (why the frick did nothing change)
            - Train a bit faster
            - Not able to repeat the test of tiny transformer paper because
            a lot of resources are needed to repeat the results of the paper
            - How we do the masking. Reflecting the behavior of CNN to convert transformer to CNN.
			-->

            We first repeat the test in <a href="#ref_9">[9]</a>, comparing the performances of a T2T-ViT-14 transformer
            <a href="#ref_7">[7]</a> to that of a CNN with the same complexity using the CiFAR-10 and CiFAR-100 datasets 
			(Training and test datasets contains 50000 and 10000 images respectively). We aim to reconfirm and quantify 
			the level of improvement with CNN over transformers in the context of these smaller datasets. <br><br>
            
			We then create a Convbrid transformer with the exact convolution characteristics of the CNN (similar
            layers and kernel sizes) and compare its performance on the CiFAR-100 dataset with the two aforementioned models.
			We expect the Convbrid model to perform similarly if not better than the CNN.<br><br>

            After this initial step, we aim to explore different ways of manipulating the Convbrid kernel sizes, and 
			analyse how they affect performance, expanding to larger datasets such as ImageNet-1K <a href="#ref_10">[10]</a><br>.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Results</h1>
			
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Conclusion</h1>
			Let's end with some discussion of the implications and limitations.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:16px">References:</span><br><br>
				<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">
					An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>,
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
                    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 2021<br><br>
				<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2006.04768">
                    Linformer: Self-Attention with Linear Complexity</a>,
                    S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, 2020<br><br>
				<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1904.10509">
                    Generating Long Sequences with Sparse Transformer</a>,
                    R. Child, S. Gray, A. Radford, I. Sutskever, 2019<br><br>
				<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/1911.02972">
                    Blockwise Self-Attention for Long Document Understanding</a>,
                    J. Qiu, H. Ma, O. Levy, S. Wen-tau Yih, S. Wang, J. Tang, 2019<br><br>
				<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2004.05150">
                    Longformer: The Long-Document Transformer</a>,
                    I. Beltagy, M. E. Peters, A. Cohan, 2020<br><br>
				<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2106.03746">
                    Efficient Training of Visual Transformers with Small Datasets</a>,
                    Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. De Nadai, 2021<br><br>
				<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/2101.11986">
                    Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</a>,
                    L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng, S. Yan, 2021<br><br>
				<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2104.05704">
                    Escaping the Big Data Paradigm with Compact Transformers</a>,
                    A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, H. Shi, 2021<br><br>
				<a id="ref_9"></a>[9] <a href="https://arxiv.org/abs/2207.10026">
                    Locality Guidance for Improving Vision Transformers on Tiny Datasets</a>,
                    K. Li, R. Yu, Z. Wang, L. Yuan, G. Song, J. Chen, 2022<br><br>
			    <a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/document/5206848">
                    Imagenet: A large-scale hierarchical image database</a>,
                    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 2009<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>
