<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}
	</style>

	<title>ConvBrid Transformer: Enforcing Locality via CNN-like Attention Masking</title>
	<meta property="og:title" content="The Platonic Representation Hypothesis" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							ConvBrid Transformer: Enforcing Locality via CNN-like Attention Masking
						</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px">Vinh Tran</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Joel Tan</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Toya Takahashi</span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#convbrid">ConvBrid Architecture</a><br><br>
				<a href="#experiments">Experiments</a><br><br>
				<a href="#results">Results</a><br><br>
				<a href="#further_work">Further Work</a><br><br>
				<a href="#conclusion">Conclusion</a><br><br>
			</div>
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Introduction</h2>
            Vision Transformers (ViTs) adapt the standard transformer architecture, originally designed for natural
            language processing, to computer vision tasks. They process images by dividing them into patches and treating
            these patches as a sequence of inputs to the transformer. ViTs have demonstrated improved performance compared
            to deep convolutional neural networks (CNNs) on large-scale datasets such as ImageNet-21k and JFM-300M. <a href="#ref_1">[1]</a>
            <br><br>

            The self-attention mechanism in Vision Transformers (ViTs) enables them to capture global image context,
            enhancing performance. However, unlike convolutional neural networks (CNNs), ViTs lack inductive biases for learning
            local structures and are not inherently translation invariant due to their reliance on positional embeddings. As
            a result, ViTs require large datasets or extensive data augmentation for effective training. This also leads to
            a higher parameter count and the need for substantially larger training datasets to match the performance of CNNs.
            Notably, when trained on mid-sized datasets without regularization, ViTs perform comparably or slightly worse
            than ResNets of similar size <a href="#ref_1">[1]</a>. When trained on CIFAR-100, ViTs tend to overfit the training set due to their difficulty in capturing local information, resulting in lower test accuracy <a href="#ref_2">[2]</a>:<br><br>

            <img src="./images/vit_vs_cnn_li.jpg" width=512px/>

            While substantial research has focused on developing accurate models that require large datasets,
            relatively little attention has been given to training models on small datasets. This poses a
            significant challenge for developing effective ViT models in domains with limited
            large-scale datasets, such as medical imaging <a href="#ref_3">[3]</a>.<br></br>

            In this project, the tendency of ViTs to overfit and struggle with learning local information was validated
            on small-scale datasets, using CIFAR-10 and CIFAR-100. To address this issue, we develop the ConvBrid
            Transformer. This is a custom hybrid CNN-ViT architecture that incorporates
            locality inductive biases. During the initial stages of training, ConvBrid uses attention masks to limit attention
            to local patches, mimicking the locality behavior of CNNs. This constraint encourages the model to learn spatial locality
            early on during training without the need of large datasets for the model to learn inductive biases. As training progresses,
			the attention masks are removed, allowing the model to learn global relationships. 
            By combining the spatial awareness of convolutional layers with the global attention capabilities of
            transformers, this design facilitates more efficient training on smaller datasets.<br><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="bg_and_rel_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background and Related Work</h1>
            Some prior work has aimed to address the lack of inductive bias in Vision Transformers (ViTs). For instance, Li
            et al. proposed adding a small multi-layer perceptron (MLP) to the architecture to penalize pairs of token embeddings
            with large relative distances <a href="#ref_4">[4]</a>. Since the MLP functions independently of the original
            architecture, it is inherently architecture-agnostic. Results show that this additional loss consistently enhances
            the accuracy of ViTs across all tested datasets, with improvements ranging from modest to significant. However,
            a notable limitation is that the MLP constrains the transformer from fully leveraging its capacity to learn
            global relationships. By being active throughout the training process, the MLP may inhibit the ViT's ability
            to exploit its full potential for modeling long-range dependencies.<br><br>

            Another approach involves using a small CNN as a regularizer to guide the local feature learning of Vision Transformers
            (ViTs) <a href="#ref_2">[2]</a>. In this method, a lightweight, pre-trained CNN operates in parallel with the downsampled
            image alongside the transformer layers. After each transformer layer, the architecture minimizes the L2 loss between the
            output of the CNN and that of the transformer. This process distills locality knowledge from the CNN into the ViT’s hidden
            layers during training, with the CNN outputs being discarded during inference. Consistent with previous studies, results
            show that on small datasets such as CIFAR-100, this approach leads to up to a 13% improvement over the baseline
            transformer model.<br><br>

            Lastly, a notable advancement in tackling this challenge is the T2T-ViT (Tokens-to-Token Vision Transformer) architecture,
            which combines MLPs for progressive tokenization of image structures with an efficient CNN-inspired
            deep-narrow backbone to model global attention relations on tokens <a href="#ref_3">[3]</a>. This dual approach enables the T2T-ViT
            to effectively capture local image details while simultaneously leveraging global context through attention mechanisms.
            By combining these capabilities, the T2T-ViT demonstrates superior or comparable performance to CNNs of similar complexity,
            such as ResNets and MobileNets. This highlights the importance of incorporating locality guidance into Vision Transformers,
            addressing their inherent lack of inductive bias and improving their efficiency in learning from midsize datasets.<br><br>

            Compared to previous methods, our ConvBrid architecture offers a more direct imitation of CNNs by initially restricting
            attention to local patches. However, as training progresses, the attention masks are removed, allowing the model to
            integrate global information alongside the local features learned in the earlier stages of training. Similar attention
            masking approaches have been proposed in <a href="#ref_2">[5-8]</a>. However, in these works, the primary focus is
            on reducing computational complexity and memory requirements when training models on large datasets. We hypothesize that
            this training approach will enable the transformer to learn local features more quickly than traditional ViTs,
            without compromising its ability to capture global dependencies, balancing between efficiently learning
            locality and maintaining the transformer's capacity for global attention. This method reflects the learning transfer approach, 
			albeit more complicated, as it allows different architectures to learn from each other. <br><br>

		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);">
		    <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="convbrid">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>ConvBrid Architecture</h2>
			
            The ConvBrid transformer is designed to emulate the behavior of traditional CNNs by extracting data from an
            image patch through convolution with a given kernel size. To achieve this, we developed the ConvAttention block,
            which is a modified attention mechanism that operates in a localized manner, similar to the small receptive field
            of a convolutional kernel. The ConvAttention block enforces this locality by masking the attention scores of tokens,
            ensuring that each token can only attend to its local "neighborhood," with each token representing a pixel
            in the image.<br><br>

            Just as in a convolutional layer, the kernel size and stride can be selected, determining the size of the "neighborhood"
            each token attends to. If 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>k</mi>
			</math>
			is the kernel size, each token 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>t</mi>
			</math>
			interacts with nearby tokens 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>N</mi><mo>(</mo><mi>t</mi><mo>)</mo>
			</math>, which correspond to the patches in its 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>k</mi>
				<mo>&#xD7;</mo>
				<mi>k</mi>
			</math>
			neighborhood including padding tokens. After the masked self-attention and an additonal residual layer, a pooling layer combines the 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<msup>
					<mi>k</mi>
					<mn>2</mn>
				</msup>
			</math>
			tokens within each convolution kernel into a single token. This pooling is implemented as a linear layer,
            followed by an MLP. Throughout the rest of this discussion, a ConvAttention layer refers to one attention
            layer, including the pooling layer and MLP. The architecture is visualized below. <br><br>

			<img src="./images/model.png" width="450px">

            Although the goal of ConvAttention is to mimic the behavior of convolutional layers, this architecture
            fundamentally differs from CNNs in that it dynamically determines the importance of interactions within
            the kernel, rather than applying fixed, learned filters as in CNNs. This flexibility allows the
            ConvAttention layer to adapt its focus based on the input patches and potentially generalize globally
            across the entire image. Additionally, since the architecture learns localized characteristics through
            convolution-like attention restrictions, positional embeddings may become unnecessary. <br><br>

            To leverage the strengths of both Vision Transformers (ViTs), in global context understanding, and CNNs,
            in capturing local patterns, ConvBrid starts training with a small kernel size. After several epochs
            (which can be configured), the attention masks are removed, allowing the model to function
            like a transformer. We also explore a more dynamic strategy of transitioning through a gradual
            relaxation of the kernel size and pooling characteristics, which could offer smoother adaptation to
            global contexts.

		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="experiments">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Experiments</h2>
			<h1>Verifying the CNN and Transformer</h1>
            Initially, a series of experiments were conducted to validate the findings of previous papers that ViTs struggle to
            find meaningful relationships between image patches that are close to each other and tend to overfit to the test data.<br><br>
			
            The test in <a href="#ref_2">[2]</a> is repeated by comparing the performance of a transformer
            <a href="#ref_3">[3]</a> to that of a benchmark CNN (described bellow) with equivalent complexity using the CIFAR-10 and CIFAR-100
            datasets. These are small datasets, with both the training and test sets containing 50,000 and 10,000
            images, respectively. The aim is to quantify the improvement level with CNNs over transformers in
            the context of small datasets and confirm that the CNN will outperform the transformer. <br><br>

            To enable meaningful comparisons, we develop a convolution block with the same complexity and structure as
            the ConvAttention block. This convolution block consists of a convolution layer combined with an MLP, followed by
            a residual layer, a max pooling layer, and a final MLP. The kernel size of the convolution layer is set to
            match that of the ConvAttention block. However, the stride and padding are selected such that the dimensions
            (or the number of tokens) are conserved. The actual stride and padding in the ConvAttention block are implemented
            through the max pooling layer. The architecture is visualized above. <br><br>

            The benchmark models are designed to allow the creation of equivalent architectures using ConvAttention or CNN blocks. The
            networks consist of an initial embedding layer, implemented using Torch's built-in Conv2d layer with a kernel
            size of 3, stride, and padding of 1. This directly maps 1024 pixels to 1024 initial tokens. The token embedding
            size is set to 64 and remains consistent throughout the layers (as the number of input and output channels for
            the convolutional layers). The tokens are then passed through 6 convolutional blocks as described below. <br><br>
			
            <table style="border-collapse: collapse; width: 40%;">
                <thead>
                    <tr>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Layer</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Kernel Size</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Stride</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Padding</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Output Size</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">16 × 16</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">3</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">4</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">4 × 4</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">6</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2 × 2</td>
                    </tr>
                </tbody>
            </table><br><br>
			Each model is run until saturation of validating accuracy, using the AdamW optimizer and Cosine Annealing scheduler.
			Each model was trained on CIFAR-10 for 40 epochs and CIFAR-100 for 150 epochs. The number of epochs was determined by
			how long it takes for the training accuracy to plateau, which is determined by when 2 epochs do not change
            training accuracy by 1%. <br><br>

			<h1>ConvAttention v.s. CNN</h1>
			To verify the functionality of the ConvAttention block, we compare two benchmark models, one created using the CNN block, and the other created using the ConvAttention. We expect
			similar behaviors and performance between the two. We further confirm the result by visualizing the attention probability of the first attention layer in the ConvAttention model 
			and inspecting where the model is "focusing" the most on in the validating images. <br><br>

			<h1>ConvBrid Transformer</h1>
            A ConvBrid transformer is then created with the exact convolutional characteristics of the CNN
            (matching layers and kernel sizes) and its performance is compared with the other three models on the
            CIFAR-10 and CIFAR-100 datasets. During the training of the ConvBrid model, the transition from ConvAttention
            (with kernel size = 5) to a full transformer (without masking) is triggered when the ConvAttention module reaches
            saturation, which is indicated by the training accuracy nearly plateauing. For the CIFAR-10 and
            CIFAR-100 datasets, this transition occurs at roughly epochs 15 and 80, respectively.

		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Results</h2>

			<h1>Evaluating the CNN and Transformer</h1>

			<img src="./images/training_stats.png" width=900px/>

            The figure above illustrates the performance of various models on CIFAR-10 and CIFAR-100, showing the upper bounds of the 
            validation accuracies. This upper bound visualization is needed as the Cosine Annealing scheduler induces fluctuations
			in the losses and accuracies, which although does not affect the performances of the models, have prooved to be distracting. 
			From the curve, we confirm the findings of previous studies that CNNs outperform transformers, achieving up to 
			86% accuracy compared to the transformer’s 80%. Furthermore, our model using ConvAttention layers closely 
			aligns with the CNN results, particularly for the larger CIFAR-100 model. This provides evidence 
			that ConvAttention is functioning as expected, effectively mimicking CNN behavior. However, it was
            surprising that the ConvBrid model did not perform better than ConvAttention, which we will discuss further in the ConvBrid
            results section.<br><br>

			<h1>Traditional ViT</h1>
			<img src="./images/transformer_attn.png" width=850px/>

            Traditional ViTs clearly exhibit overfitting to the data, as shown above. For instance, in the image of the car,
            the tokens focus on the red background rather than the car itself, indicating poor generalization. This overfitting
            is further highlighted in the previous graph, where the validation accuracy for the traditional transformer plateaus
            at an earlier epoch compared to the other models. As predicted, ViTs require a larger dataset to effectively learn and
            capture fundamental local features, which they struggle to extract when trained on smaller datasets.<br><br>
			
			<h1>ConvAttention</h1>
			<img src="./images/conv_attention_visualization_cifar10.png" width=850px/>

            In addition to comparing the validation loss and accuracy curves, we visualized the average attention map for the
            first ConvAttention layer to confirm its behavior. As seen above, the attention map closely resembles the behavior
            of a convolutional layer, effectively enforcing locality, which demonstrates that the attention masking is functioning
            as intended. Higher attention is given to tokens near edges or regions with abrupt color transitions. This suggests that
            the attention mechanism is naturally adept at detecting fine-grained local features, such as corners and edges, similar
            to a CNN. This behavior highlights the model’s ability to capture local patterns early in the training process.<br><br>

			<h1>ConvBrid</h1>
			<img src="./images/convbrid_attn_.png" width=850px/>

            From the attention maps of our ConvBrid model, we discovered that the switching mechanism from CNN to a
            transformer was not functioning as intended, and the model was still learning locality features. Even without
            the attention masking, ConvAttention continued to learn patterns such as edges and corners through the pooling
            layer, which combines tokens within each kernel via a linear layer. When comparing the attention map with the
            previous ConvAttention implementation of a CNN, we observed the same behavior.<br><br>

            In another experiment, we removed the attention mask from the first epoch, but the same attention patterns emerged,
            albeit with empirically faster training times. This suggests an interesting result: attention masking may be useful
            for improving training speed but is not strictly necessary, due to the pooling layer. This creates a challenge in
            transitioning between ConvAttention and a traditional transformer, ultimately leading to ConvBrid achieving the same
            results as ConvAttention.
			</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="further_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Further Work</h2>
            A further attempt was made to adapt the ConvBrid kernel sizes more fluidly during training. In this modified model,
            the kernel size is defined as a function of the epoch, allowing it to grow gradually. This setup enables the model
            to progressively learn broader contextual information, mimicking a hierarchical learning process that is potentially
            more efficient.<br><br>

			However, because the pooling layer in ConvAttention requires a fixed structure across epochs, it must be configured
            to accommodate the maximum kernel size anticipated during training. While in the static case, the input dimension
            of the pooling layer is <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>, in this dynamic approach it must be set to <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>max_kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>. 
            If the current kernel size is smaller than the maximum, tokens outside the kernel remain unutilized during the
            training pass. To address this, a custom backward pass was implemented in the linear pooling layer to enable
            selective dropout of these tokens during updates. <br><br>

			As of now, the model set with a kernel size of 5 achieved only 52% accuracy, which suggests further debugging is
            necessary. Theoretically, this dynamic kernel approach should perform comparably to or better than the initial
            ConvBrid model, given its ability to progressively expand its receptive field during training.		
        </div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Conclusion</h2>
            This study validates the limitations of traditional Vision Transformers (ViTs) on small-scale datasets and
            introduces the ConvBrid Transformer, a hybrid architecture designed to address these challenges by incorporating
            CNN-like locality inductive biases. By leveraging attention masking in the early stages of training and
            transitioning to global attention in later epochs, ConvBrid seeks to balance efficient learning of local
            features with the global context capabilities of ViTs. <br><br>

            The experiments confirmed that CNNs outperform ViTs on small datasets like CIFAR-10 and CIFAR-100 due to their
            ability to capture local patterns effectively. The ConvAttention block demonstrated promising results by
            mimicking CNN behavior, providing evidence that locality-guided attention can enhance ViT performance on
            small datasets. However, the ConvBrid model’s transition mechanism from ConvAttention to a full transformer
            revealed challenges, with the model failing to outperform ConvAttention alone. This was attributed to the
            pooling layer's inherent ability to capture local features, which limited the impact of transitioning to global
            attention. <br><br>

            Further exploration into dynamic kernel sizes for ConvBrid highlighted potential avenues for improvement, such
            as hierarchical learning strategies to progressively broaden contextual understanding. However, these approaches
            require further refinement, as initial attempts achieved suboptimal accuracy, emphasizing the need for additional
            debugging and optimization. <br><br>

            Moving forward, future work will focus on fine-tuning kernel size manipulation strategies, evaluating their impact
            on performance, and scaling experiments to larger datasets like ImageNet-1K <a href="#ref_9">[9]</a>. This progression aims to solidify
            ConvBrid's potential as a robust solution for adapting ViTs to domains with limited data, bridging the gap between
            CNN and transformer paradigms.
        </div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:16px">References:</span><br><br>
				<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">
					An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>,
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
                    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 2021<br><br>
				<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2207.10026">
                    Locality Guidance for Improving Vision Transformers on Tiny Datasets</a>,
                    K. Li, R. Yu, Z. Wang, L. Yuan, G. Song, J. Chen, 2022<br><br>
				<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/2101.11986">
                    Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</a>,
                    L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng, S. Yan, 2021<br><br>
				<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2106.03746">
                    Efficient Training of Visual Transformers with Small Datasets</a>,
                    Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. De Nadai, 2021<br><br>
				<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2006.04768">
                    Linformer: Self-Attention with Linear Complexity</a>,
                    S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, 2020<br><br>
				<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/1904.10509">
                    Generating Long Sequences with Sparse Transformer</a>,
                    R. Child, S. Gray, A. Radford, I. Sutskever, 2019<br><br>
				<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/1911.02972">
                    Blockwise Self-Attention for Long Document Understanding</a>,
                    J. Qiu, H. Ma, O. Levy, S. Wen-tau Yih, S. Wang, J. Tang, 2019<br><br>
				<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2004.05150">
                    Longformer: The Long-Document Transformer</a>,
                    I. Beltagy, M. E. Peters, A. Cohan, 2020<br><br>
			    <a id="ref_9"></a>[9] <a href="https://ieeexplore.ieee.org/document/5206848">
                    Imagenet: A large-scale hierarchical image database</a>,
                    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 2009<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>
