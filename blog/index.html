<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}
	</style>

	<title>ConvBrid Transformer: Enforcing Locality via CNN-like Attention Masking</title>
	<meta property="og:title" content="The Platonic Representation Hypothesis" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							ConvBrid Transformer: Enforcing Locality via CNN-like Attention Masking
						</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px">Vinh Tran</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Joel Tan</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Toya Takahashi</span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#convbrid">ConvBrid Architecture</a><br><br>
				<a href="#experiments">Experiments</a><br><br>
				<a href="#results">Results</a><br><br>
				<a href="#further_work">Further Work</a><br><br>
				<a href="#conclusion">Conclusion</a><br><br>
			</div>
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Introduction</h2>
            Vision Transformers (ViTs) adapt the standard transformer architecture, originally designed for natural
            language processing, to computer vision tasks. They process images by dividing them into patches and treating
            these patches as a sequence of inputs to the transformer. ViTs have demonstrated improved performance compared
            to deep convolutional neural networks (CNNs) on large-scale datasets such as ImageNet-21k and JFM-300M. <a href="#ref_1">[1]</a>
            <br><br>

            The self-attention mechanism in Vision Transformers (ViTs) enables them to capture global image context,
            enhancing performance. However, unlike convolutional neural networks (CNNs), ViTs lack inductive biases for learning
            local structures and are not inherently translation invariant due to their reliance on positional embeddings. As
            a result, ViTs require large datasets or extensive data augmentation for effective training. This also leads to
            a higher parameter count and the need for substantially larger training datasets to match the performance of CNNs.
            Notably, when trained on mid-sized datasets without regularization, ViTs perform comparably or slightly worse
            than ResNets of similar size <a href="#ref_1">[1]</a>. When trained on CIFAR-100, ViTs tend to overfit the training set due to their difficulty in capturing local information, resulting in lower test accuracy <a href="#ref_9">[9]</a>:<br><br>

            <img src="./images/vit_vs_cnn_li.jpg" width=512px/>

            While substantial research has focused on developing accurate models that require large datasets,
            relatively little attention has been given to training models on small datasets. This poses a
            significant challenge for developing effective ViT models in domains with limited
            large-scale datasets, such as medical imaging <a href="#ref_7">[7]</a>.<br></br>

            In this project, the tendency of ViTs to overfit and struggle with learning local information was validated
            on small-scale datasets, using CIFAR-10 and CIFAR-100. Then, to address this issue, the ConvBrid
            Transformer was developed. This is a custom hybrid CNN-ViT architecture that incorporates
            locality inductive biases. During the initial stages of training, ConvBrid uses attention masks to limit attention
            to local patches, mimicking the locality behavior of CNNs. This constraint encourages the model to learn spatial locality
            early on during training without the need of large datasets for the model to learn inductive biases.
            By combining the spatial awareness of convolutional layers with the global attention capabilities of
            transformers, this design facilitates more efficient training on smaller datasets.<br><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="bg_and_rel_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background and Related Work</h1>
            Some prior work has aimed to address the lack of inductive bias in Vision Transformers (ViTs). For instance, Li
            et al. proposed adding a small multi-layer perceptron (MLP) to the architecture to penalize pairs of token embeddings
            with large relative distances <a href="#ref_6">[6]</a>. Since the MLP functions independently of the original
            architecture, it is inherently architecture-agnostic. Results show that this additional loss consistently enhances
            the accuracy of ViTs across all tested datasets, with improvements ranging from modest to significant. However,
            a notable limitation is that the MLP constrains the transformer from fully leveraging its capacity to learn
            global relationships. By being active throughout the training process, the MLP may inhibit the ViT's ability
            to exploit its full potential for modeling long-range dependencies.<br><br>

            Another approach involves using a small CNN as a regularizer to guide the local feature learning of Vision Transformers
            (ViTs) <a href="#ref_9">[9]</a>. In this method, a lightweight, pre-trained CNN operates in parallel with the downsampled
            image alongside the transformer layers. After each transformer layer, the architecture minimizes the L2 loss between the
            output of the CNN and that of the transformer. This process distills locality knowledge from the CNN into the ViT’s hidden
            layers during training, with the CNN outputs being discarded during inference. Consistent with previous studies, results
            show that on small datasets such as CIFAR-100, this approach leads to up to a 13% improvement over the baseline transformer model.
            <br><br>

            Lastly, a notable advancement in tackling this challenge is the T2T-ViT (Tokens-to-Token Vision Transformer) architecture,
            which combines MLPs for progressive tokenization of image structures with an efficient CNN-inspired
            deep-narrow backbone to model global attention relations on tokens <a href="#ref_7">[7]</a>. This dual approach enables the T2T-ViT
            to effectively capture local image details while simultaneously leveraging global context through attention mechanisms.
            By combining these capabilities, the T2T-ViT demonstrates superior or comparable performance to CNNs of similar complexity,
            such as ResNets and MobileNets. This highlights the importance of incorporating locality guidance into Vision Transformers,
            addressing their inherent lack of inductive bias and improving their efficiency in learning from midsize datasets.<br><br>

            Compared to previous methods, our ConvBrid architecture offers a more direct imitation of CNNs by initially restricting
            attention to local patches. However, as training progresses, the attention masks are removed, allowing the model to
            integrate global information alongside the local features learned in the earlier stages of training. We hypothesize that
            this novel training approach will enable the transformer to learn local features more quickly than traditional ViTs,
            without compromising its ability to capture global dependencies. This method aims to find a balance between efficiently learning
            locality and maintaining the transformer's capacity for global attention.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);">
		    <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="convbrid">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>ConvBrid Architecture</h2>
			
            The ConvBrid transformer is designed to emulate the behavior of traditional CNNs by extracting data from an
            image patch through convolution with a given kernel size. To achieve this, we developed the ConvAttention block,
            which is a modified attention mechanism that operates in a localized manner, similar to the small receptive field
            of a convolutional kernel. The ConvAttention block enforces this locality by masking the attention scores of tokens,
            ensuring that each token can only attend to its local "neighborhood," with each token representing a pixel
            in the image.<br><br>

            Just as in a convolutional layer, the kernel size and stride can be selected, determining the size of the "neighborhood"
            each token attends to. If 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>k</mi>
			</math>
			is the kernel size, each token 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>t</mi>
			</math>
			interacts with nearby tokens 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>N</mi><mo>(</mo><mi>t</mi><mo>)</mo>
			</math>, which correspond to the patches in its 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>k</mi>
				<mo>&#xD7;</mo>
				<mi>k</mi>
			</math>
			neighborhood including padding tokens. After the masked self-attention and an addiitonal residual layer, a pooling layer combines the 
			<math xmlns="http://www.w3.org/1998/Math/MathML">
				<msup>
					<mi>k</mi>
					<mn>2</mn>
				</msup>
			</math>
			tokens within each convolution kernel into a single token. This pooling is implemented as a linear layer,
            followed by an MLP. Throughout the rest of this discussion, a ConvAttention layer refers to one attention
            layer, including the pooling layer and MLP. The architecture is visualized below. <br><br>

			<img src="./images/model.png" width="450px">

            Although the goal of ConvAttention is to mimic the behavior of convolutional layers, this architecture
            fundamentally differs from CNNs in that it dynamically determines the importance of interactions within
            the kernel, rather than applying fixed, learned filters as in CNNs. This flexibility allows the
            ConvAttention layer to adapt its focus based on the input patches and potentially generalize globally
            across the entire image. Additionally, since the architecture learns localized characteristics through
            convolution-like attention restrictions, positional embeddings may become unnecessary. <br><br>

            To leverage the strengths of both Vision Transformers (ViTs), in global context understanding, and CNNs,
            in capturing local patterns, ConvBrid starts training with a small kernel size. After several epochs
            (which can be configured), the attention masks are removed, allowing the model to function
            like a transformer. We also explore a more dynamic strategy of transitioning through a gradual
            relaxation of the kernel size and pooling characteristics, which could offer smoother adaptation to
            global contexts.

		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="experiments">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Experiments</h2>
			<h1>Verifying the CNN and Transformer</h1>
            Initially, a series of experiments were conducted to validate the findings of previous papers that ViTs struggle to
            find meaningful relationships between image patches that are close to each other and tend to overfit to the test data.<br><br>
			
            The test in <a href="#ref_9">[9]</a> is repeated by comparing the performance of a transformer
            <a href="#ref_7">[7]</a> to that of a CNN with equivalent complexity using the CIFAR-10 and CIFAR-100
            datasets. These are small datasets, with both the training and test sets containing 50,000 and 10,000
            images, respectively. The aim is to quantify the improvement level with CNNs over transformers in
            the context of small datasets and confirm that the CNN will outperform the transformer. <br><br>

			In order to have meaningful comparisons, we develop a convolution block with the same complexity and structure 
			as the ConvAttention block. This convolution block composes of a convolution layer combined with a MLP, following 
			by a residual layer, a max pooling layer, and a final MLP. The kernel size of the convolution layer is set to be 
			the same as in the ConvAttention block. The stride and padding, however, are choosen such that the dimention 
			(or the number of tokens) is conserved. The actual stride and padding of the ConvAttention block are implemented via the
			max pooling layer. <br></br>

            The benchmark CNN is designed to allow the creation of equivalent architectures using ConvAttentionlayers. The
            networks consist of an initial embedding layer, implemented using Torch's built-in Conv2D layers with a kernel
            size of 3, stride, and padding of 1. This directly maps 1024 pixels to 1024 initial tokens. The token embedding
            size is set to 64 and remains consistent throughout the layers (as the number of input and output channels for
            the convolutional layers). The tokens are then passed through 6 convolutional blocks as described below. For the
            CNN, we built the convolution blocks with two simple convolutional layers, with MLP layers in between and afterward.
            This ensures that the traditional convolutional block performs with the same complexity as the ConvAttention block. <br><br>
			
            <table style="border-collapse: collapse; width: 100%;">
                <thead>
                    <tr>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Layer</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Kernel Size</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Stride</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Padding</th>
                        <th style="padding: 10px; border: 1px solid black; text-align: center;">Output Size</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">16 × 16</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">3</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">4</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">1</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">8 × 8</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">4 × 4</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">6</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">5</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2</td>
                        <td style="padding: 10px; border: 1px solid black; text-align: center;">2 × 2</td>
                    </tr>
                </tbody>
            </table><br><br>
			Each model is run until saturation of validating accuracy, using the AdamW optimizer and Cosine Annealing scheduler. <br><br>

			<h1>ConvAttention v.s. CNN</h1>
			To verify the functionality of the ConvAttention layer, models created using ConvAttention and CNN layers with
            the same complexity (explain what same complexity mean) ar compared. <br><br>

			We except similar behaviour and validation accuracy. Although ConvAttention take more time to train. <br><br>

			Discuss the attention visualization of the ConvAttention model <br><br>

			<h1>ConvBrid Transformer</h1>
			A ConvBrid transformer was then created with the exact convolution characteristics of the CNN (similar
            layers and kernel sizes) and compare its performance on the CIFAR-10 and CIFAR-100 datasets with the two aforementioned models. 
			The ConvBrid model is expected to perform similarly if not better than the CNN.<br><br>

            After this initial step, the aim is to explore different ways of manipulating the ConvBrid kernel sizes, and 
			analyse how they affect performance, expanding to larger datasets such as ImageNet-1K <a href="#ref_10">[10]</a><br>.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Results</h2>

			<h1>Evaluating the CNN and Transformer</h1>
            TODO MOVE THIS TO EXPERIMENTS
			Each model was trained on CIFAR-10 for 40 epochs and CIFAR-100 for 150 epochs. The number of epochs was determined by
			how long it takes for the training accuracy to plateau, which is determined by when 2 epochs do not change
			training accuracy by 1%. 

			<img src="./images/training_stats.png" width=900px/>

            The figure above illustrates the performance of various models on CIFAR-10 and CIFAR-100, showing the best
            validation accuracy achieved up to each point. From the curve, we confirm the findings of previous studies that
            CNNs outperform transformers, achieving up to 88% accuracy compared to the transformer’s 80%. Furthermore, our model
            using ConvAttention layers closely aligns with the CNN results, particularly for the larger CIFAR-100 model. This
            provides evidence that ConvAttention is functioning as expected, effectively mimicking CNN behavior. However, it was
            surprising that the ConvBrid model did not perform better than ConvAttention, which we will discuss further in the ConvBrid
            results section.<br><br>

			<h1>Traditional ViT</h1>
			<img src="./images/transformer_attn.png" width=850px/>

            Traditional ViTs clearly exhibit overfitting to the data, as shown above. For instance, in the image of the car,
            the tokens focus on the red background rather than the car itself, indicating poor generalization. This overfitting
            is further highlighted in the previous graph, where the validation accuracy for the traditional transformer plateaus
            at an earlier epoch compared to the other models. As predicted, ViTs require a larger dataset to effectively learn and
            capture fundamental local features, which they struggle to extract when trained on smaller datasets.<br><br>
			
			<h1>ConvAttention</h1>
			<img src="./images/conv_attention_visualization_cifar10.png" width=850px/>

            In addition to comparing the validation loss and accuracy curves, we visualized the average attention map for the
            first ConvAttention layer to confirm its behavior. As seen above, the attention map closely resembles the behavior
            of a convolutional layer, effectively enforcing locality, which demonstrates that the attention masking is functioning
            as intended. Higher attention is given to tokens near edges or regions with abrupt color transitions. This suggests that
            the attention mechanism is naturally adept at detecting fine-grained local features, such as corners and edges, similar
            to a CNN. This behavior highlights the model’s ability to capture local patterns early in the training process.<br><br>

			<h1>ConvBrid</h1>
			<img src="./images/convbrid_attn_.png" width=850px/>

            From the attention maps for our ConvBrid model, we found that the switching mechanism from CNN to a transformer was not working as
            intended. 

            Discuss the TradAttention model, explaining that is the ConvAttention without attention masking (but with pooling). This model behavae exactly like the ConvAttention.
			Discuss that the restricting mechanism is satisfy with the pooling alone and the attention masking is not necessary. This is still an interesting result. However, it create a diffuculty in transitioning between ConvAttention and traditional transfomer. <br></br>

			During the training of the ConvBrid model, the transition from ConvAttention (with kernel size = 5) to a transformer (without masking) 
			is triggered when the ConvAttention module reaches saturation, indicated by the training accuracy nearly plateauing. For CIFAR-10 and 
			CIFAR-100, this corresponds to epochs 15 and 80 respectively, as represented in the figure above.<br></br>

			</div>
		<div class="margin-right-block">
		</div>
	</div>

	<!-- REMOVE IF THERES ENOUGH THINGS TO SAY-->
	<div class="content-margin-container" id="further_work">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Further Work</h2>
			A further attempt was to change the ConvBrid kernel sizes more fluidly during the training process. The changed ConvBrid
			model takes as input a function from epoch to kernel size, so that by inputting a slowly growing function,
			the model will learn progressively broader contextual information. It aims to mimic a hierarchical learning process which
			will be more efficient. <br></br>

			However, as the pooling layer in the ConvAttention needs to have a fixed structure across epochs, it has to be initially set
			to be compatible with the maximum kernel size in the training process. While in the static case the dimension of the 
			input is <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>, now it has to be set to <math xmlns="http://www.w3.org/1998/Math/MathML">
				<mi>max_kernel</mi>
				<msup>
					<mi></mi>
					<mn>2</mn>
				</msup>
				<mo>&#xD7;</mo>
				<mi>num_channels</mi>
			</math>, if the current kernel size
			is less than the corresponding weights, the tokens outside the kernel will not be updated in the training pass. Thus, a custom
			backward pass in the linear pooling layer was implemented to enable the layer to do this selective dropout. <br></br>
			
			As of now, the model set at kernel = 5 is only able to reach 52% accuracy. This probably points to further debugging that has to be
			done. Theoretically, it should be able to perform comparably to the initial ConvBrid model, if not better. <br></br>
			
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h2>Conclusion</h2>
			Let's end with some discussion of the implications and limitations.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:16px">References:</span><br><br>
				<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">
					An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>,
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
                    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 2021<br><br>
				<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2006.04768">
                    Linformer: Self-Attention with Linear Complexity</a>,
                    S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, 2020<br><br>
				<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1904.10509">
                    Generating Long Sequences with Sparse Transformer</a>,
                    R. Child, S. Gray, A. Radford, I. Sutskever, 2019<br><br>
				<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/1911.02972">
                    Blockwise Self-Attention for Long Document Understanding</a>,
                    J. Qiu, H. Ma, O. Levy, S. Wen-tau Yih, S. Wang, J. Tang, 2019<br><br>
				<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2004.05150">
                    Longformer: The Long-Document Transformer</a>,
                    I. Beltagy, M. E. Peters, A. Cohan, 2020<br><br>
				<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2106.03746">
                    Efficient Training of Visual Transformers with Small Datasets</a>,
                    Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. De Nadai, 2021<br><br>
				<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/2101.11986">
                    Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</a>,
                    L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng, S. Yan, 2021<br><br>
				<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2104.05704">
                    Escaping the Big Data Paradigm with Compact Transformers</a>,
                    A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, H. Shi, 2021<br><br>
				<a id="ref_9"></a>[9] <a href="https://arxiv.org/abs/2207.10026">
                    Locality Guidance for Improving Vision Transformers on Tiny Datasets</a>,
                    K. Li, R. Yu, Z. Wang, L. Yuan, G. Song, J. Chen, 2022<br><br>
			    <a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/document/5206848">
                    Imagenet: A large-scale hierarchical image database</a>,
                    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 2009<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>
