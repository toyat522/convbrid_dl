<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			#"Avenir";
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}
	</style>

	<title>CONVbrid Transformer: Enforcing Locality via CNN-like Attention Masking</title>
	<meta property="og:title" content="The Platonic Representation Hypothesis" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							CONVbrid Transformer: Enforcing Locality via CNN-like Attention Masking
						</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px">Vinh Tran</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Joel Tan</span>
					</td>
					<td align=left>
						<span style="font-size:17px">Toya Takahashi</span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<!-- table of contents here -->
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#does_x_do_y">Does X do Y?</a><br><br>
				<a href="#implications_and_limitations">Implications and limitations</a><br><br>
			</div>
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Introduction</h1>
			Vision Transformers (ViTs) are applications of the standard transformer to computer vision settings,
			drawing inspiration from the success of these architectures in natural language processing. When applied to
			images, they divide the image into patches and use the sequence of patches as an input into the transformer. 
			<a href="#ref_1">[1]</a> They have been shown to perform better than deep CNNs on key datasets such as ImageNet
            and CIFAR-10/100.<br><br>

            To address the challenge of ViTs requiring large datasets for effective training, we propose the CONVbrid
            Transformer, a hybrid CNN-ViT architecture that incorporates locality inductive biases. This design enables more
            efficient training of transformers on smaller datasets by leveraging the spatial awareness of convolutional
            layers alongside the global attention of transformers. During the initial N epochs of training, CONVbrid uses
            attention masks to limit attention to local patches, mimicking CNN behavior. This constraint encourages the
            model to learn spatial locality early on, enhancing data efficiency and robustness on smaller datasets. Similar
            attention masking approaches have been proposed in [e.g. 2–5]. However, in these works, the authors focus on
            reducing complexity and memory requirements in training models with large datasets. CONVbrid also introduces
            a parameter λ during these epochs, controlling the rate at which this constraint is relaxed. With a larger λ,
            the attention masks expand more quickly, enabling a gradual transition toward global attention.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="does_x_do_y">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background and Related Work</h1>
            Prior work has been done on mitigating the lack of inductive bias. Adding a small multi-layer perception (MLP)
            to the architecture to learn relative distances in the image from the patch embeddings <a href="#ref_6">[6]</a> has been
            shown to improve performance. Using MLPs to successively tokenize the image is another way to capture local
            structures <a href="#ref_7">[7]</a>. CONVbrid will provide a closer imitation of the CNNs with the desirable inductive
            bias. It has also been shown that careful tokenization can overcome the need for large datasets in
            transformers. <a href="#ref_8">[8]</a><br><br>

			Now let's write some math!<br>
			<center>
				<math xmlns="http://www.w3.org/1998/Math/MathML">
					<mrow>
						<mrow>
							<mo>&#x2202;</mo>
							<mi>y</mi>
						</mrow>
						<mo>/</mo>
						<mrow>
							<mo>&#x2202;</mo>
							<mi>x</mi>
						</mrow>
					</mrow>
					<mo>=</mo>
					<mi>x</mi>
				</math>
			</center>
			<br>
			It's probably best to ask an LLM to help do the web
			formatting for math. You can tell it "convert this latex equation into MathML: $$\frac{\partial dy}{\partial
			dx} = x$$"
			But it took me a few tries. So, if you get frustrated, you can embed an image of the equation, or use other
			packages for
			rendering equations on webpages.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);">
		    <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Experiments</h1>
            We will perform a series of experiments to determine the performance of our architecture on various small
            datasets and compare the results with pure CNNs and ViTs.<br><br>

            We first repeat the test in <a href="#ref_9">[9]</a>, comparing the performances of a T2T-ViT-14 transformer
            <a href="#ref_7">[7]</a> to that of a CNN with the same complexity using the CiFAR-100 dataset. We aim to
            reconfirm and quantify the level of improvement with CNN over transformers in the context of smaller datasets.
            We then create a CONVbrid transformer with the exact convolution characteristics of the CNN (similar
            layers and kernel sizes) and compare its performance on the CiFAR-100 dataset with the two aforementioned models.
            We expect the CONVbrid model to perform similarly if not better than the CNN.<br><br>

            After this initial step, we will explore how the following factors affect performance, expanding to
            larger datasets such as ImageNet-1K <a href="#ref_10">[10]</a><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Results</h1>
			Let's end with some discussion of the implications and limitations.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="implications_and_limitations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Conclusion</h1>
			Let's end with some discussion of the implications and limitations.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:16px">References:</span><br><br>
				<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">
					An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>,
                    A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
                    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 2021<br><br>
				<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2006.04768">
                    Linformer: Self-Attention with Linear Complexity</a>,
                    S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, 2020<br><br>
				<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1904.10509">
                    Generating Long Sequences with Sparse Transformer</a>,
                    R. Child, S. Gray, A. Radford, I. Sutskever, 2019<br><br>
				<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/1911.02972">
                    Blockwise Self-Attention for Long Document Understanding</a>,
                    J. Qiu, H. Ma, O. Levy, S. Wen-tau Yih, S. Wang, J. Tang, 2019<br><br>
				<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2004.05150">
                    Longformer: The Long-Document Transformer</a>,
                    I. Beltagy, M. E. Peters, A. Cohan, 2020<br><br>
				<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2106.03746">
                    Efficient Training of Visual Transformers with Small Datasets</a>,
                    Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. De Nadai, 2021<br><br>
				<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/2101.11986">
                    Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</a>,
                    L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng, S. Yan, 2021<br><br>
				<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2104.05704">
                    Escaping the Big Data Paradigm with Compact Transformers</a>,
                    A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, H. Shi, 2021<br><br>
				<a id="ref_9"></a>[9] <a href="https://arxiv.org/abs/2207.10026">
                    Locality Guidance for Improving Vision Transformers on Tiny Datasets</a>,
                    K. Li, R. Yu, Z. Wang, L. Yuan, G. Song, J. Chen, 2022<br><br>
			    <a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/document/5206848">
                    Imagenet: A large-scale hierarchical image database</a>,
                    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 2009<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>
